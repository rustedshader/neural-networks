{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e84ad21",
   "metadata": {},
   "source": [
    "# Vectorization \n",
    "\n",
    "Converting Text to vector consisting of numbers.\n",
    "\n",
    "2 words -> bigram\n",
    "3 words -> tigram\n",
    "\n",
    "\n",
    "Text to vectorization -> Identify all the tokens and assign one vector to the token\n",
    "\n",
    "One of the simplest way is One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6a797",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "One-hot encoding is a technique that converts categorical data into numerical data. It's used in machine learning to prepare data for algorithms and improve predictions. \n",
    "\n",
    "## How it works \n",
    "- Create a new binary column for each category\n",
    "- Mark the presence of a category with a 1 in the corresponding column\n",
    "- Mark all other categories with a 0\n",
    "\n",
    "\n",
    "## In NLP\n",
    "A one-hot vector is a 1 Ã— N matrix (vector) used to distinguish each word in a vocabulary from every other word in the vocabulary\n",
    "\n",
    "One-hot encoding ensures that machine learning does not assume that higher numbers are more important.\n",
    "\n",
    "For example, the value '8' is bigger than the value '1', but that does not make '8' more important than '1'. The same is true for words: the value 'laughter' is not more important than 'laugh'.\n",
    "\n",
    "## In Machine Learning\n",
    "one-hot encoding is a frequently used method to deal with categorical data. Because many machine learning models need their input variables to be numeric, categorical variables need to be transformed in the pre-processing part.\n",
    "\n",
    "Categorical data can be either nominal or ordinal.[7] Ordinal data has a ranked order for its values and can therefore be converted to numerical data through ordinal encoding.[8] An example of ordinal data would be the ratings on a test ranging from A to F, which could be ranked using numbers from 6 to 1. Since there is no quantitative relationship between nominal variables' individual values, using ordinal encoding can potentially create a fictional ordinal relationship in the data.[9] Therefore, one-hot encoding is often applied to nominal variables, in order to improve the performance of the algorithm.\n",
    "\n",
    "### Disadvantage\n",
    "Because this process creates multiple new variables, it is prone to creating a 'big p' problem (too many predictors) if there are many unique values in the original column. Another downside of one-hot encoding is that it causes multicollinearity between the individual variables, which potentially reduces the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfeb6d9",
   "metadata": {},
   "source": [
    "![img](https://i.sstatic.net/vhQC3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643982bd",
   "metadata": {},
   "source": [
    "# Text to sequence\n",
    "\n",
    "Strings into list of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94950175",
   "metadata": {},
   "source": [
    "# Fit on Text\n",
    "fit_on_texts used in conjunction with texts_to_matrix produces the one-hot encoding for a text\n",
    "texts_to_matrix: Convert a list of texts to a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20d4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras._tf_keras.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "text=['Hello Shubhang Shubhang Hello Hi Wassup Shubhang','Shubhang is hello world']\n",
    "\n",
    "test_tokenizer = Tokenizer()\n",
    "# Building Word Index\n",
    "test_tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66975819",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = test_tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6391d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 unique tokens\n",
      "{'shubhang': 1, 'hello': 2, 'hi': 3, 'wassup': 4, 'is': 5, 'world': 6}\n"
     ]
    }
   ],
   "source": [
    "one_hot_result = test_tokenizer.texts_to_matrix(text,mode='binary')\n",
    "word_index = test_tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fce54f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0b4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b89fbe",
   "metadata": {},
   "source": [
    "# OOV -> Out of Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c18fd",
   "metadata": {},
   "source": [
    "# Pad Sequences\n",
    "\n",
    "pad_sequences is used to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e189f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 1, 2, 3, 4, 1], [1, 5, 2, 6]]\n",
      "\n",
      "[[2 1 1 2 3 4 1]\n",
      " [0 0 0 1 5 2 6]]\n"
     ]
    }
   ],
   "source": [
    "from keras._tf_keras.keras.preprocessing.sequence import pad_sequences\n",
    "print(sequences)\n",
    "print()\n",
    "# Here you can see 000 are added to make same length\n",
    "print(pad_sequences(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d1403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
